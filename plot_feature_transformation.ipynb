{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Feature transformations with ensembles of trees\n",
    "\n",
    "\n",
    "Transform your features into a higher dimensional, sparse space. Then\n",
    "train a linear model on these features.\n",
    "\n",
    "First fit an ensemble of trees (totally random trees, a random\n",
    "forest, or gradient boosted trees) on the training set. Then each leaf\n",
    "of each tree in the ensemble is assigned a fixed arbitrary feature\n",
    "index in a new feature space. These leaf indices are then encoded in a\n",
    "one-hot fashion.\n",
    "\n",
    "Each sample goes through the decisions of each tree of the ensemble\n",
    "and ends up in one leaf per tree. The sample is encoded by setting\n",
    "feature values for these leaves to 1 and the other feature values to 0.\n",
    "\n",
    "The resulting transformer has then learned a supervised, sparse,\n",
    "high-dimensional categorical embedding of the data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Tim Head <betatim@gmail.com>\n",
    "#\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(10)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import (RandomTreesEmbedding, RandomForestClassifier,\n",
    "                              GradientBoostingClassifier)\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "n_estimator = 10\n",
    "X, y = make_classification(n_samples=80000)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n",
    "\n",
    "# It is important to train the ensemble of trees on a different subset\n",
    "# of the training data than the linear regression model to avoid\n",
    "# overfitting, in particular if the total number of leaves is\n",
    "# similar to the number of training samples\n",
    "X_train, X_train_lr, y_train, y_train_lr = train_test_split(\n",
    "    X_train, y_train, test_size=0.5)\n",
    "\n",
    "# Unsupervised transformation based on totally random trees\n",
    "rt = RandomTreesEmbedding(max_depth=3, n_estimators=n_estimator,\n",
    "                          random_state=0)\n",
    "\n",
    "rt_lm = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "pipeline = make_pipeline(rt, rt_lm)\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred_rt = pipeline.predict_proba(X_test)[:, 1]\n",
    "fpr_rt_lm, tpr_rt_lm, _ = roc_curve(y_test, y_pred_rt)\n",
    "\n",
    "# Supervised transformation based on random forests\n",
    "rf = RandomForestClassifier(max_depth=3, n_estimators=n_estimator)\n",
    "rf_enc = OneHotEncoder(categories='auto')\n",
    "rf_lm = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_enc.fit(rf.apply(X_train))\n",
    "rf_lm.fit(rf_enc.transform(rf.apply(X_train_lr)), y_train_lr)\n",
    "\n",
    "y_pred_rf_lm = rf_lm.predict_proba(rf_enc.transform(rf.apply(X_test)))[:, 1]\n",
    "fpr_rf_lm, tpr_rf_lm, _ = roc_curve(y_test, y_pred_rf_lm)\n",
    "\n",
    "# Supervised transformation based on gradient boosted trees\n",
    "grd = GradientBoostingClassifier(n_estimators=n_estimator)\n",
    "grd_enc = OneHotEncoder(categories='auto')\n",
    "grd_lm = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "grd.fit(X_train, y_train)\n",
    "grd_enc.fit(grd.apply(X_train)[:, :, 0])\n",
    "grd_lm.fit(grd_enc.transform(grd.apply(X_train_lr)[:, :, 0]), y_train_lr)\n",
    "\n",
    "y_pred_grd_lm = grd_lm.predict_proba(\n",
    "    grd_enc.transform(grd.apply(X_test)[:, :, 0]))[:, 1]\n",
    "fpr_grd_lm, tpr_grd_lm, _ = roc_curve(y_test, y_pred_grd_lm)\n",
    "\n",
    "# The gradient boosted model by itself\n",
    "y_pred_grd = grd.predict_proba(X_test)[:, 1]\n",
    "fpr_grd, tpr_grd, _ = roc_curve(y_test, y_pred_grd)\n",
    "\n",
    "# The random forest model by itself\n",
    "y_pred_rf = rf.predict_proba(X_test)[:, 1]\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_rf)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_rt_lm, tpr_rt_lm, label='RT + LR')\n",
    "plt.plot(fpr_rf, tpr_rf, label='RF')\n",
    "plt.plot(fpr_rf_lm, tpr_rf_lm, label='RF + LR')\n",
    "plt.plot(fpr_grd, tpr_grd, label='GBT')\n",
    "plt.plot(fpr_grd_lm, tpr_grd_lm, label='GBT + LR')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(2)\n",
    "plt.xlim(0, 0.2)\n",
    "plt.ylim(0.8, 1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_rt_lm, tpr_rt_lm, label='RT + LR')\n",
    "plt.plot(fpr_rf, tpr_rf, label='RF')\n",
    "plt.plot(fpr_rf_lm, tpr_rf_lm, label='RF + LR')\n",
    "plt.plot(fpr_grd, tpr_grd, label='GBT')\n",
    "plt.plot(fpr_grd_lm, tpr_grd_lm, label='GBT + LR')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve (zoomed in at top left)')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
